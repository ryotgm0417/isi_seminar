{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(threshold=100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# データの読み込み（Iris データセット）\n",
    "data_pd = pd.read_csv('data/iris.data', sep=',', header=None)\n",
    "data_size = len(data_pd)\n",
    "\n",
    "data = data_pd.iloc[:,0:4].to_numpy()\n",
    "t = data_pd.iloc[:,4].to_numpy()       # 正解のカテゴリ\n",
    "\n",
    "t = OneHotEncoder(sparse=False).fit_transform(t.reshape(-1, 1)) # カテゴリをOne-Hot-Encodingする"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RATIO_TRAIN = 0.8\n",
    "\n",
    "random_order_list = np.random.permutation(data_size) # データの順番をランダムにする\n",
    "num_train = int(data_size * RATIO_TRAIN)\n",
    "\n",
    "train_data = data[ random_order_list[:num_train] ] # 訓練データ\n",
    "train_target = t[ random_order_list[:num_train] ]\n",
    "\n",
    "test_data = data[ random_order_list[num_train:] ] # テストデータ\n",
    "test_target = t[ random_order_list[num_train:] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mech-user/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/mech-user/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み（MNIST手書き文字）\n",
    "train_df = pd.read_csv('data/mnist-in-csv/mnist_train.csv', sep=',')\n",
    "test_df = pd.read_csv('data/mnist-in-csv/mnist_test.csv', sep=',')\n",
    "\n",
    "train_data = train_df.iloc[:,1:].to_numpy(dtype='float32')\n",
    "train_target = train_df.iloc[:,0].to_numpy(dtype='int8')\n",
    "train_target = OneHotEncoder(sparse=False).fit_transform(train_target.reshape(-1, 1))\n",
    "\n",
    "test_data = test_df.iloc[:,1:].to_numpy(dtype='float32')\n",
    "test_target = test_df.iloc[:,0].to_numpy(dtype='int8')\n",
    "test_target = OneHotEncoder(sparse=False).fit_transform(test_target.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重要パラメータの定義\n",
    "# データの性質関連\n",
    "TRAIN_SIZE = 20000       # 訓練データ数\n",
    "TEST_SIZE = 2000         # テストデータ数\n",
    "STEPS_PER_DATA = 10      # 1つのデータを何ステップ入れ続けるか\n",
    "NUM_INPUT_NODES = 784    # 入力の次元数\n",
    "NUM_OUTPUT_NODES = 10    # 出力の次元数\n",
    "\n",
    "# リザバー関連\n",
    "LEAK_RATE=0.4\n",
    "SPECTRAL_RADIUS = 0.3\n",
    "NUM_RESERVOIR_NODES = 1000 # リザバー内のユニット数\n",
    "# BIAS = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像をいっぺんに（784次元のまま）入力する場合\n",
    "train_data = train_data[:TRAIN_SIZE]\n",
    "test_data = test_data[:TEST_SIZE]\n",
    "train_target = train_target[:TRAIN_SIZE]\n",
    "test_target = test_target[:TEST_SIZE]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 画像を短冊状にして（28行に分けて）入力する場合\n",
    "train_data = train_data[:TRAIN_SIZE].reshape((-1,28))\n",
    "test_data = test_data[:TEST_SIZE].reshape((-1,28))\n",
    "train_target = train_target[:TRAIN_SIZE]\n",
    "test_target = test_target[:TEST_SIZE]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 短冊状にしたものを、元画像が20x20となるようにトリミング\n",
    "\n",
    "NUM_INPUT_NODES = 20\n",
    "STEPS_PER_DATA = 20\n",
    "\n",
    "train_indices = np.zeros(20*TRAIN_SIZE, dtype='int')\n",
    "for i in range(TRAIN_SIZE):\n",
    "    train_indices[i*20 : (i+1)*20] = np.arange((i*28+4),(i*28+24))\n",
    "    \n",
    "train_data = train_data[train_indices,4:24]\n",
    "\n",
    "test_indices = np.zeros(20*TEST_SIZE, dtype='int')\n",
    "for i in range(TEST_SIZE):\n",
    "    test_indices[i*20 : (i+1)*20] = np.arange((i*28+4),(i*28+24))\n",
    "    \n",
    "test_data = test_data[test_indices,4:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像をいっぺんに（784次元のまま）入力する場合\n",
    "train_data = np.array([i for i in train_data for _ in range(STEPS_PER_DATA)])\n",
    "test_data = np.array([i for i in test_data for _ in range(STEPS_PER_DATA)])\n",
    "train_target = np.array([i for i in train_target for _ in range(STEPS_PER_DATA)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 画像を短冊状にして（28行に分けて）入力する場合\n",
    "train_target = [i for i in test_target for _ in range(STEPS_PER_DATA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数の別例（利用しない）\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# ある行の最大値を1、それ以外を0にする関数\n",
    "def max_to_one(row):\n",
    "    index = np.argmax(row)\n",
    "    row = np.zeros(len(row))\n",
    "    row[index] = 1\n",
    "    return row\n",
    "\n",
    "# ***この関数は不要になった***\n",
    "# parentは2次元numpy配列で、childが足したい行（1次元numpy配列）\n",
    "# np.append(parent, [child], axis=0)に等しい\n",
    "# def numpy_append(parent, child):\n",
    "#     parent_list = parent.tolist()\n",
    "#     child_list = child.tolist()\n",
    "#     parent_list.append(child_list)\n",
    "#     parent = np.asarray(parent_list)\n",
    "#     return parent\n",
    "\n",
    "class ReservoirNetWork: # 入れるtrain_data, test_data, targetは全てnumpy配列\n",
    "\n",
    "    def __init__(self, train_data, num_input_nodes, num_reservoir_nodes, num_output_nodes, leak_rate=0.1, activator=np.tanh):\n",
    "        self.train_data = train_data\n",
    "        self.log_reservoir_nodes = np.zeros((len(train_data), num_reservoir_nodes), dtype='float32') # reservoir層のノードの状態を記録\n",
    "\n",
    "        # init weights\n",
    "        self.weights_input = self._generate_input_weights(num_input_nodes, num_reservoir_nodes).astype('float32')\n",
    "        self.weights_reservoir = self._generate_reservoir_weights(num_reservoir_nodes).astype('float32')\n",
    "        self.weights_output = np.zeros((num_reservoir_nodes, num_output_nodes), dtype='float32')\n",
    "\n",
    "        # それぞれの層のノードの数\n",
    "        self.num_input_nodes = num_input_nodes\n",
    "        self.num_reservoir_nodes = num_reservoir_nodes\n",
    "        self.num_output_nodes = num_output_nodes\n",
    "\n",
    "        self.leak_rate = leak_rate # 漏れ率\n",
    "        self.activator = activator # 活性化関数\n",
    "\n",
    "    # reservoir層のノードの次の状態を取得\n",
    "    def _get_next_reservoir_nodes(self, input, current_state):\n",
    "        next_state = (1 - self.leak_rate) * current_state\n",
    "        next_state += self.leak_rate * (input @ self.weights_input + current_state @ self.weights_reservoir)\n",
    "        return self.activator(next_state)\n",
    "\n",
    "    # 出力層の重みを更新\n",
    "    def _update_weights_output(self, target, lambda0):\n",
    "        # Ridge Regression\n",
    "        E_lambda0 = np.identity(self.num_reservoir_nodes) * lambda0\n",
    "        inv_x = np.linalg.inv(self.log_reservoir_nodes.T @ self.log_reservoir_nodes + E_lambda0)\n",
    "        # update weights of output layer\n",
    "        self.weights_output = ((inv_x @ self.log_reservoir_nodes.T) @ target)\n",
    "\n",
    "        \n",
    "    # 学習する\n",
    "    def train(self, lambda0=0.1):\n",
    "        \n",
    "        # 入力を1つずつ入れていく\n",
    "        for i in range(len(self.train_data)):\n",
    "            tr = self.train_data[i]\n",
    "            current_state = self.log_reservoir_nodes[max(0,i-1)]\n",
    "                \n",
    "            self.log_reservoir_nodes[i] = self._get_next_reservoir_nodes(tr, current_state)   # 内部状態更新\n",
    "            \n",
    "            # 元々はappendを利用して書いていたが、遅いからやめた\n",
    "            # self.log_reservoir_nodes = numpy_append(self.log_reservoir_nodes, self._get_next_reservoir_nodes(tr, current_state))\n",
    "            # self.log_reservoir_nodes = np.append(self.log_reservoir_nodes, [self._get_next_reservoir_nodes(tr, current_state)], axis=0)\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print('training data no. {}\\n'.format(i))\n",
    "                \n",
    "        # まとめて行列計算で重みを更新\n",
    "        self._update_weights_output(train_target, lambda0)\n",
    "        \n",
    "        \n",
    "    # 予測する\n",
    "    def predict(self, test_data):\n",
    "        reservoir_nodes = self.log_reservoir_nodes[-1] # 訓練の結果得た最後の内部状態を取得\n",
    "        \n",
    "        # 空の配列を用意\n",
    "        predict_output = np.zeros((len(test_data), (NUM_OUTPUT_NODES)), dtype='float32')\n",
    "        reduced_predict_output = np.zeros(( int(len(test_data)/STEPS_PER_DATA), NUM_OUTPUT_NODES), dtype='float32')\n",
    "        tmp_array = np.zeros(NUM_OUTPUT_NODES)\n",
    "        tmp_count = 0\n",
    "        \n",
    "        # 入力を1つずつ入れていく\n",
    "        for i in range(len(test_data)):\n",
    "            te = test_data[i]\n",
    "            reservoir_nodes = self._get_next_reservoir_nodes(te, reservoir_nodes) # 内部状態更新\n",
    "            predict_output[i] = max_to_one( self.get_output(reservoir_nodes) ) # 内部状態を読み出して出力を得る、出力が最大のところを1とする\n",
    "            # predict_output = numpy_append(predict_output, output)\n",
    "            # predict_output = np.append(predict_output, [output], axis=0)\n",
    "            # predict_output = predict_output[1:]\n",
    "            \n",
    "            tmp_array += predict_output[i]\n",
    "            \n",
    "            # 画像1枚分の入力が終わるごとに、出力結果を記録。tmp_countは「何枚目の画像」かを示す\n",
    "            if(i%STEPS_PER_DATA == STEPS_PER_DATA - 1):\n",
    "                reduced_predict_output[tmp_count] = tmp_array\n",
    "                # reduced_predict_output = numpy_append(reduced_predict_output, tmp_array)\n",
    "                # reduced_predict_output = np.append(reduced_predict_output, [tmp_array], axis=0)\n",
    "                tmp_array = np.zeros(NUM_OUTPUT_NODES)\n",
    "                tmp_count += 1    \n",
    "                \n",
    "        # 各画像について、多数決で出力を決める\n",
    "        final_predict_output = [max_to_one(row) for row in reduced_predict_output]\n",
    "        \n",
    "        return final_predict_output, predict_output, reduced_predict_output\n",
    "\n",
    "    \n",
    "    # 内部状態から出力を計算\n",
    "    def get_output(self, reservoir_nodes):\n",
    "        return reservoir_nodes @ self.weights_output \n",
    "\n",
    "    #############################\n",
    "    ##### private method ########\n",
    "    #############################\n",
    "\n",
    "    # 入力層の重み：0.1か0か-0.1で初期化したものを返す\n",
    "    def _generate_input_weights(self, num_input_nodes, num_reservoir_nodes):\n",
    "        return np.random.choice([-0.1, 0, 0.1], num_input_nodes*num_reservoir_nodes, p=[0.1, 0.8, 0.1]).reshape(num_input_nodes, num_reservoir_nodes)\n",
    "\n",
    "    # Reservoir層の重みを初期化\n",
    "    def _generate_reservoir_weights(self, num_nodes):\n",
    "        # 0以上1以下のランダムな値\n",
    "        weights = np.random.normal(0, 1, num_nodes * num_nodes)\n",
    "        \n",
    "        # ランダムに90%を選び、0とする（疎な行列にする）\n",
    "        indices = np.random.choice(np.arange(len(weights)), replace=False, size=int(len(weights) * 0.9))\n",
    "        weights[indices] = 0\n",
    "        \n",
    "        weights = weights.reshape([num_nodes, num_nodes])\n",
    "        spectral_radius = max(abs(linalg.eigvals(weights)))\n",
    "        return weights / spectral_radius * SPECTRAL_RADIUS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data no. 0\n",
      "\n",
      "training data no. 1000\n",
      "\n",
      "training data no. 2000\n",
      "\n",
      "training data no. 3000\n",
      "\n",
      "training data no. 4000\n",
      "\n",
      "training data no. 5000\n",
      "\n",
      "training data no. 6000\n",
      "\n",
      "training data no. 7000\n",
      "\n",
      "training data no. 8000\n",
      "\n",
      "training data no. 9000\n",
      "\n",
      "training data no. 10000\n",
      "\n",
      "training data no. 11000\n",
      "\n",
      "training data no. 12000\n",
      "\n",
      "training data no. 13000\n",
      "\n",
      "training data no. 14000\n",
      "\n",
      "training data no. 15000\n",
      "\n",
      "training data no. 16000\n",
      "\n",
      "training data no. 17000\n",
      "\n",
      "training data no. 18000\n",
      "\n",
      "training data no. 19000\n",
      "\n",
      "training data no. 20000\n",
      "\n",
      "training data no. 21000\n",
      "\n",
      "training data no. 22000\n",
      "\n",
      "training data no. 23000\n",
      "\n",
      "training data no. 24000\n",
      "\n",
      "training data no. 25000\n",
      "\n",
      "training data no. 26000\n",
      "\n",
      "training data no. 27000\n",
      "\n",
      "training data no. 28000\n",
      "\n",
      "training data no. 29000\n",
      "\n",
      "training data no. 30000\n",
      "\n",
      "training data no. 31000\n",
      "\n",
      "training data no. 32000\n",
      "\n",
      "training data no. 33000\n",
      "\n",
      "training data no. 34000\n",
      "\n",
      "training data no. 35000\n",
      "\n",
      "training data no. 36000\n",
      "\n",
      "training data no. 37000\n",
      "\n",
      "training data no. 38000\n",
      "\n",
      "training data no. 39000\n",
      "\n",
      "training data no. 40000\n",
      "\n",
      "training data no. 41000\n",
      "\n",
      "training data no. 42000\n",
      "\n",
      "training data no. 43000\n",
      "\n",
      "training data no. 44000\n",
      "\n",
      "training data no. 45000\n",
      "\n",
      "training data no. 46000\n",
      "\n",
      "training data no. 47000\n",
      "\n",
      "training data no. 48000\n",
      "\n",
      "training data no. 49000\n",
      "\n",
      "training data no. 50000\n",
      "\n",
      "training data no. 51000\n",
      "\n",
      "training data no. 52000\n",
      "\n",
      "training data no. 53000\n",
      "\n",
      "training data no. 54000\n",
      "\n",
      "training data no. 55000\n",
      "\n",
      "training data no. 56000\n",
      "\n",
      "training data no. 57000\n",
      "\n",
      "training data no. 58000\n",
      "\n",
      "training data no. 59000\n",
      "\n",
      "training data no. 60000\n",
      "\n",
      "training data no. 61000\n",
      "\n",
      "training data no. 62000\n",
      "\n",
      "training data no. 63000\n",
      "\n",
      "training data no. 64000\n",
      "\n",
      "training data no. 65000\n",
      "\n",
      "training data no. 66000\n",
      "\n",
      "training data no. 67000\n",
      "\n",
      "training data no. 68000\n",
      "\n",
      "training data no. 69000\n",
      "\n",
      "training data no. 70000\n",
      "\n",
      "training data no. 71000\n",
      "\n",
      "training data no. 72000\n",
      "\n",
      "training data no. 73000\n",
      "\n",
      "training data no. 74000\n",
      "\n",
      "training data no. 75000\n",
      "\n",
      "training data no. 76000\n",
      "\n",
      "training data no. 77000\n",
      "\n",
      "training data no. 78000\n",
      "\n",
      "training data no. 79000\n",
      "\n",
      "training data no. 80000\n",
      "\n",
      "training data no. 81000\n",
      "\n",
      "training data no. 82000\n",
      "\n",
      "training data no. 83000\n",
      "\n",
      "training data no. 84000\n",
      "\n",
      "training data no. 85000\n",
      "\n",
      "training data no. 86000\n",
      "\n",
      "training data no. 87000\n",
      "\n",
      "training data no. 88000\n",
      "\n",
      "training data no. 89000\n",
      "\n",
      "training data no. 90000\n",
      "\n",
      "training data no. 91000\n",
      "\n",
      "training data no. 92000\n",
      "\n",
      "training data no. 93000\n",
      "\n",
      "training data no. 94000\n",
      "\n",
      "training data no. 95000\n",
      "\n",
      "training data no. 96000\n",
      "\n",
      "training data no. 97000\n",
      "\n",
      "training data no. 98000\n",
      "\n",
      "training data no. 99000\n",
      "\n",
      "training data no. 100000\n",
      "\n",
      "training data no. 101000\n",
      "\n",
      "training data no. 102000\n",
      "\n",
      "training data no. 103000\n",
      "\n",
      "training data no. 104000\n",
      "\n",
      "training data no. 105000\n",
      "\n",
      "training data no. 106000\n",
      "\n",
      "training data no. 107000\n",
      "\n",
      "training data no. 108000\n",
      "\n",
      "training data no. 109000\n",
      "\n",
      "training data no. 110000\n",
      "\n",
      "training data no. 111000\n",
      "\n",
      "training data no. 112000\n",
      "\n",
      "training data no. 113000\n",
      "\n",
      "training data no. 114000\n",
      "\n",
      "training data no. 115000\n",
      "\n",
      "training data no. 116000\n",
      "\n",
      "training data no. 117000\n",
      "\n",
      "training data no. 118000\n",
      "\n",
      "training data no. 119000\n",
      "\n",
      "training data no. 120000\n",
      "\n",
      "training data no. 121000\n",
      "\n",
      "training data no. 122000\n",
      "\n",
      "training data no. 123000\n",
      "\n",
      "training data no. 124000\n",
      "\n",
      "training data no. 125000\n",
      "\n",
      "training data no. 126000\n",
      "\n",
      "training data no. 127000\n",
      "\n",
      "training data no. 128000\n",
      "\n",
      "training data no. 129000\n",
      "\n",
      "training data no. 130000\n",
      "\n",
      "training data no. 131000\n",
      "\n",
      "training data no. 132000\n",
      "\n",
      "training data no. 133000\n",
      "\n",
      "training data no. 134000\n",
      "\n",
      "training data no. 135000\n",
      "\n",
      "training data no. 136000\n",
      "\n",
      "training data no. 137000\n",
      "\n",
      "training data no. 138000\n",
      "\n",
      "training data no. 139000\n",
      "\n",
      "training data no. 140000\n",
      "\n",
      "training data no. 141000\n",
      "\n",
      "training data no. 142000\n",
      "\n",
      "training data no. 143000\n",
      "\n",
      "training data no. 144000\n",
      "\n",
      "training data no. 145000\n",
      "\n",
      "training data no. 146000\n",
      "\n",
      "training data no. 147000\n",
      "\n",
      "training data no. 148000\n",
      "\n",
      "training data no. 149000\n",
      "\n",
      "training data no. 150000\n",
      "\n",
      "training data no. 151000\n",
      "\n",
      "training data no. 152000\n",
      "\n",
      "training data no. 153000\n",
      "\n",
      "training data no. 154000\n",
      "\n",
      "training data no. 155000\n",
      "\n",
      "training data no. 156000\n",
      "\n",
      "training data no. 157000\n",
      "\n",
      "training data no. 158000\n",
      "\n",
      "training data no. 159000\n",
      "\n",
      "training data no. 160000\n",
      "\n",
      "training data no. 161000\n",
      "\n",
      "training data no. 162000\n",
      "\n",
      "training data no. 163000\n",
      "\n",
      "training data no. 164000\n",
      "\n",
      "training data no. 165000\n",
      "\n",
      "training data no. 166000\n",
      "\n",
      "training data no. 167000\n",
      "\n",
      "training data no. 168000\n",
      "\n",
      "training data no. 169000\n",
      "\n",
      "training data no. 170000\n",
      "\n",
      "training data no. 171000\n",
      "\n",
      "training data no. 172000\n",
      "\n",
      "training data no. 173000\n",
      "\n",
      "training data no. 174000\n",
      "\n",
      "training data no. 175000\n",
      "\n",
      "training data no. 176000\n",
      "\n",
      "training data no. 177000\n",
      "\n",
      "training data no. 178000\n",
      "\n",
      "training data no. 179000\n",
      "\n",
      "training data no. 180000\n",
      "\n",
      "training data no. 181000\n",
      "\n",
      "training data no. 182000\n",
      "\n",
      "training data no. 183000\n",
      "\n",
      "training data no. 184000\n",
      "\n",
      "training data no. 185000\n",
      "\n",
      "training data no. 186000\n",
      "\n",
      "training data no. 187000\n",
      "\n",
      "training data no. 188000\n",
      "\n",
      "training data no. 189000\n",
      "\n",
      "training data no. 190000\n",
      "\n",
      "training data no. 191000\n",
      "\n",
      "training data no. 192000\n",
      "\n",
      "training data no. 193000\n",
      "\n",
      "training data no. 194000\n",
      "\n",
      "training data no. 195000\n",
      "\n",
      "training data no. 196000\n",
      "\n",
      "training data no. 197000\n",
      "\n",
      "training data no. 198000\n",
      "\n",
      "training data no. 199000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# モデルの定義\n",
    "model = ReservoirNetWork(train_data=train_data,\n",
    "    num_input_nodes=NUM_INPUT_NODES,\n",
    "    num_reservoir_nodes=NUM_RESERVOIR_NODES,\n",
    "    num_output_nodes=NUM_OUTPUT_NODES,\n",
    "    leak_rate=LEAK_RATE)\n",
    "\n",
    "model.train() # 訓練\n",
    "\n",
    "# 予測結果\n",
    "final_predict_output, predict_output, reduced_predict_output = model.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1165"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 誤差\n",
    "np.sum(np.absolute(final_predict_output - test_target)) * 0.5 / len(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正解のデータ\n",
    "len(test_target.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 予測値（画像ごと）\n",
    "len(final_predict_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各ステップごとの予測値\n",
    "len(predict_output.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 画像ごとの多数決の結果\n",
    "len(reduced_predict_output.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mech-user/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/mech-user/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 以下、比較対象とするために線形回帰とRidge回帰も試してみる\n",
    "# データ読み込み\n",
    "train_df = pd.read_csv('data/mnist-in-csv/mnist_train.csv', sep=',')\n",
    "test_df = pd.read_csv('data/mnist-in-csv/mnist_test.csv', sep=',')\n",
    "\n",
    "train_data = train_df.iloc[:,1:].to_numpy(dtype='float32')\n",
    "train_target = train_df.iloc[:,0].to_numpy(dtype='int8')\n",
    "train_target = OneHotEncoder(sparse=False).fit_transform(train_target.reshape(-1, 1))\n",
    "\n",
    "test_data = test_df.iloc[:,1:].to_numpy(dtype='float32')\n",
    "test_target = test_df.iloc[:,0].to_numpy(dtype='int8')\n",
    "test_target = OneHotEncoder(sparse=False).fit_transform(test_target.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:TRAIN_SIZE]\n",
    "test_data = test_data[:TEST_SIZE]\n",
    "train_target = train_target[:TRAIN_SIZE]\n",
    "test_target = test_target[:TEST_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mech-user/.local/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.07117e-12): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "\n",
    "lr = LinearRegression()\n",
    "ridge = Ridge(alpha=0.1)\n",
    "\n",
    "lr.fit(train_data, train_target)\n",
    "ridge.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = lr.predict(test_data)\n",
    "pred_ridge = ridge.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred_lr)):\n",
    "    pred_lr[i] = max_to_one(pred_lr[i])\n",
    "    pred_ridge[i] = max_to_one(pred_ridge[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1895"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.absolute(pred_lr - test_target)) * 0.5 / len(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1905"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.absolute(pred_ridge - test_target)) * 0.5 / len(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
